{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd042181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_ems(ems_path):\n",
    "    \"\"\"\n",
    "    Clean EMS dispatch dataset.\n",
    "    Returns a dataframe with one row per EMS event, ready for simulation.\n",
    "    \"\"\"\n",
    "\n",
    "    ems = pd.read_csv(ems_path, dtype=str)\n",
    "\n",
    "    dt_cols = [\"Dispatch DtTm\", \"Response DtTm\", \"On Scene DtTm\",\n",
    "               \"Received DtTm\", \"Entry DtTm\", \"Call Date\"]\n",
    "    for col in dt_cols:\n",
    "        if col in ems.columns:\n",
    "            ems[col] = pd.to_datetime(ems[col], errors=\"coerce\")\n",
    "\n",
    "    def parse_lon(s):\n",
    "        if isinstance(s, str) and s.startswith(\"POINT\"):\n",
    "            try:\n",
    "                return float(s.split(\"(\")[1].split(\" \")[0])\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    def parse_lat(s):\n",
    "        if isinstance(s, str) and s.startswith(\"POINT\"):\n",
    "            try:\n",
    "                return float(s.split(\"(\")[1].split(\" \")[1].replace(\")\", \"\"))\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    ems[\"lon\"] = ems[\"case_location\"].apply(parse_lon)\n",
    "    ems[\"lat\"] = ems[\"case_location\"].apply(parse_lat)\n",
    "\n",
    "   \n",
    "    if \"Unit Type\" in ems.columns:\n",
    "        ems[\"Unit Type_upper\"] = ems[\"Unit Type\"].str.upper()\n",
    "        mask_unit = ems[\"Unit Type_upper\"].str.contains(\n",
    "            \"MEDIC|AMBUL|ALS|RESCUE\", na=False\n",
    "        )\n",
    "        ems = ems[mask_unit].copy()\n",
    "        ems.drop(columns=[\"Unit Type_upper\"], inplace=True)\n",
    "\n",
    "    if \"Call Type\" in ems.columns:\n",
    "        ems = ems[ems[\"Call Type\"].str.contains(\"Medical Incident\", na=False)]\n",
    "\n",
    "    ems = ems.dropna(subset=[\"Dispatch DtTm\", \"lat\", \"lon\"])\n",
    "\n",
    "    cols = [\n",
    "        \"Call Number\",\n",
    "        \"Incident Number\",\n",
    "        \"Call Type\",\n",
    "        \"Call Type Group\",\n",
    "        \"Unit Type\",\n",
    "        \"Dispatch DtTm\",\n",
    "        \"Response DtTm\",\n",
    "        \"On Scene DtTm\",\n",
    "        \"Priority\",\n",
    "        \"Original Priority\",\n",
    "        \"City\",\n",
    "        \"Address\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "    ]\n",
    "    cols = [c for c in cols if c in ems.columns]\n",
    "\n",
    "    ems_events = ems[cols].copy()\n",
    "\n",
    "    ems_events = ems_events.reset_index(drop=True)\n",
    "    ems_events[\"event_id\"] = ems_events.index\n",
    "\n",
    "    return ems_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ems_events = clean_ems(\"../data/1_full_dataset/ems_logs.csv\")\n",
    "ems_events.to_csv('cleaned_emergency_logs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_pems_metadata(meta_path):\n",
    "    \"\"\"\n",
    "    Clean PeMS station metadata file.\n",
    "    Uses your actual columns: 'ID','Fwy','Dir','District','Latitude','Longitude','Type','Lanes','Name','City'\n",
    "    Returns only mainline stations in District 4 with valid coords.\n",
    "    \"\"\"\n",
    "\n",
    "    meta = pd.read_csv(meta_path, dtype=str)\n",
    "\n",
    "    col_map = {\n",
    "        \"Fwy\": \"Freeway\",\n",
    "        \"Dir\": \"Freeway Direction\",\n",
    "        \"District\": \"District\",\n",
    "        \"Latitude\": \"Latitude\",\n",
    "        \"Longitude\": \"Longitude\",\n",
    "        \"Type\": \"Type\",\n",
    "        \"Lanes\": \"Lanes\",\n",
    "        \"Name\": \"Name\",\n",
    "        \"City\": \"City\",\n",
    "    }\n",
    "\n",
    "    for raw, std in col_map.items():\n",
    "        if raw in meta.columns and std not in meta.columns:\n",
    "            meta[std] = meta[raw]\n",
    "\n",
    "    for col in [\"Latitude\", \"Longitude\", \"Lanes\", \"District\"]:\n",
    "        if col in meta.columns:\n",
    "            meta[col] = pd.to_numeric(meta[col], errors=\"coerce\")\n",
    "\n",
    "  \n",
    "    meta = meta.dropna(subset=[\"Latitude\", \"Longitude\"])\n",
    "\n",
    "   \n",
    "    if \"District\" in meta.columns:\n",
    "        meta = meta[meta[\"District\"] == 4]\n",
    "\n",
    "    if \"Type\" in meta.columns:\n",
    "        meta = meta[meta[\"Type\"] == \"ML\"]\n",
    "\n",
    "    # Final columns to keep\n",
    "    wanted = [\n",
    "        \"ID\",\n",
    "        \"Freeway\",\n",
    "        \"Freeway Direction\",\n",
    "        \"District\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "        \"Type\",\n",
    "        \"Lanes\",\n",
    "        \"Name\",\n",
    "        \"City\",\n",
    "    ]\n",
    "    keep = [c for c in wanted if c in meta.columns]\n",
    "\n",
    "    meta_clean = meta[keep].reset_index(drop=True)\n",
    "\n",
    "    return meta_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = clean_pems_metadata('../data/1_raw_data/d04_text_meta_2025_01_15.txt')\n",
    "meta_data.to_csv('../data/3_clean_dataset/cleaned_station_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92790f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BASE_COLS = [\n",
    "    \"Timestamp\",\n",
    "    \"Station\",\n",
    "    \"District\",\n",
    "    \"Freeway\",\n",
    "    \"Direction of Travel\",\n",
    "    \"Lane Type\",\n",
    "    \"Station Length\",\n",
    "    \"Samples\",\n",
    "    \"% Observed\",\n",
    "    \"Total Flow\",\n",
    "    \"Avg Occupancy\",\n",
    "    \"Avg Speed\",\n",
    "]\n",
    "\n",
    "def clean_pems_day_txt(fp):\n",
    "    \"\"\"\n",
    "    Clean ONE PeMS station_5min daily txt file.\n",
    "    - Reads as comma-separated\n",
    "    - Renames leading columns to standard names\n",
    "    - Parses Timestamp\n",
    "    - Converts key numeric columns\n",
    "    - Adds file_date\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(fp, sep=\",\", dtype=str)\n",
    "\n",
    "    # Rename leading columns\n",
    "    rename_map = {}\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if i < len(BASE_COLS):\n",
    "            rename_map[col] = BASE_COLS[i]\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # Parse timestamp\n",
    "    if \"Timestamp\" in df.columns:\n",
    "        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "    num_cols = [\"Station Length\", \"Samples\", \"% Observed\", \"Total Flow\",\n",
    "                \"Avg Occupancy\", \"Avg Speed\"]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    name = os.path.basename(fp).replace(\".txt\", \"\")\n",
    "    parts = name.split(\"_\")\n",
    "    y, m, d = map(int, parts[-3:])\n",
    "    df[\"file_date\"] = date(y, m, d)\n",
    "\n",
    "    if \"Station\" in df.columns:\n",
    "        df = df.dropna(subset=[\"Timestamp\", \"Station\"])\n",
    "    else:\n",
    "        df = df.dropna(subset=[\"Timestamp\"])\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c893e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "DATA_DIR = \"../data/1_raw_data\"\n",
    "txt_files = sorted(glob.glob(DATA_DIR + \"/d04_text_station_5min_2025_*.txt\"))\n",
    "\n",
    "pems_days_clean = []\n",
    "bad_files = []\n",
    "\n",
    "for fp in txt_files:\n",
    "    try:\n",
    "        day_df = clean_pems_day_txt(fp)\n",
    "        pems_days_clean.append(day_df)\n",
    "        print(\"cleaned:\", os.path.basename(fp), \"rows:\", len(day_df))\n",
    "    except Exception as e:\n",
    "        bad_files.append((fp, repr(e)))\n",
    "        print(\"skip:\", os.path.basename(fp), \"->\", repr(e))\n",
    "\n",
    "print(\"bad files:\", len(bad_files))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
