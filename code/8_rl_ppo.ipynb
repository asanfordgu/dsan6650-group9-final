{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_training.py\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "from integration_pems_ems_sumo import SUTrafficEnv\n",
    "\n",
    "\n",
    "class SB3TrafficEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Gym-compatible wrapper for Stable-Baselines3 (old Gym API).\n",
    "    Wraps SUTrafficEnv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sumo_cfg: str,\n",
    "        ems_day,\n",
    "        pems_day_rl,\n",
    "        meta_rl,\n",
    "        tls_ids,\n",
    "        use_gui: bool = False,\n",
    "        sim_duration_s: int = 3600,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._env = SUTrafficEnv(\n",
    "            sumo_cfg=sumo_cfg,\n",
    "            ems_day=ems_day,\n",
    "            pems_day_rl=pems_day_rl,\n",
    "            meta_rl=meta_rl,\n",
    "            tls_ids=tls_ids,\n",
    "            use_gui=use_gui,\n",
    "            sim_duration_s=sim_duration_s,\n",
    "        )\n",
    "        self.tls_ids = tls_ids\n",
    "        self.num_tls = len(tls_ids)\n",
    "        self.max_phases = 4  # adjust if your TLS has more phases\n",
    "\n",
    "        # Action space: discrete phase selection per TLS\n",
    "        if self.num_tls == 1:\n",
    "            self.action_space = spaces.Discrete(self.max_phases)\n",
    "        else:\n",
    "            self.action_space = spaces.MultiDiscrete([self.max_phases] * self.num_tls)\n",
    "\n",
    "        # Observation space: [time_of_day, hours_to_next_EV, avg_flow, avg_speed, avg_occ, phases...]\n",
    "        low = np.array(\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0] + [0.0] * self.num_tls,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        high = np.array(\n",
    "            [1.0, 24.0, 5000.0, 120.0, 1.0] + [float(self.max_phases)] * self.num_tls,\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)\n",
    "\n",
    "    def _clip_obs(self, obs):\n",
    "        obs = np.array(obs, dtype=np.float32)\n",
    "        if len(obs) > 0:\n",
    "            obs[0] = np.clip(obs[0], 0.0, 1.0)\n",
    "        if len(obs) > 1:\n",
    "            obs[1] = np.clip(obs[1], 0.0, 24.0)\n",
    "        if len(obs) > 2:\n",
    "            obs[2] = np.clip(obs[2], 0.0, 5000.0)\n",
    "        if len(obs) > 3:\n",
    "            obs[3] = np.clip(obs[3], 0.0, 120.0)\n",
    "        if len(obs) > 4:\n",
    "            obs[4] = np.clip(obs[4], 0.0, 1.0)\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self._env.reset()\n",
    "        return self._clip_obs(obs).astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.num_tls == 1:\n",
    "            a = int(action)\n",
    "        else:\n",
    "            a = np.array(action, dtype=int).tolist()\n",
    "\n",
    "        obs, reward, done, info = self._env.step(a)\n",
    "        obs = self._clip_obs(obs).astype(np.float32)\n",
    "        return obs, float(reward), bool(done), info\n",
    "\n",
    "    def close(self):\n",
    "        self._env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"gym==0.21.0\" stable-baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_training.py (continued)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "def make_env(sumo_cfg, ems_day, pems_day_rl, meta_rl, tls_ids):\n",
    "    def _init():\n",
    "        env = SB3TrafficEnv(\n",
    "            sumo_cfg=sumo_cfg,\n",
    "            ems_day=ems_day,\n",
    "            pems_day_rl=pems_day_rl,\n",
    "            meta_rl=meta_rl,\n",
    "            tls_ids=tls_ids,\n",
    "            use_gui=False,\n",
    "            sim_duration_s=3600,  # 1 simulated hour per episode\n",
    "        )\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "def train_rl_agent(\n",
    "    sumo_cfg,\n",
    "    ems_day,\n",
    "    pems_day_rl,\n",
    "    meta_rl,\n",
    "    tls_ids,\n",
    "    total_timesteps=200_000,\n",
    "    model_path=\"models/ppo_traffic.zip\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Train PPO agent on one (or a set of) EMS+PeMS day(s).\n",
    "    For multiple days, you could randomize which day/env each episode uses.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "    env_fn = make_env(sumo_cfg, ems_day, pems_day_rl, meta_rl, tls_ids)\n",
    "    vec_env = DummyVecEnv([env_fn])\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tb_logs/\",\n",
    "        n_steps=1024,\n",
    "        batch_size=256,\n",
    "        gamma=0.99,\n",
    "        learning_rate=3e-4,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    model.save(model_path)\n",
    "\n",
    "    vec_env.close()\n",
    "    print(\"Model saved to:\", model_path)\n",
    "    return model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9bad0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_eval.py\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from rl_training import SB3TrafficEnv\n",
    "from baselines.controllers import FixedTimeController, GreedyEVPreemptionController\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, episodes=5, is_sb3=False):\n",
    "    \"\"\"\n",
    "    Evaluate a policy on an env:\n",
    "    - policy: either 'sb3_model' or an object with .select_action(obs)\n",
    "    - is_sb3: True if using Stable-Baselines3 model\n",
    "    Returns list of episode rewards.\n",
    "    \"\"\"\n",
    "    ep_rewards = []\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_r = 0.0\n",
    "\n",
    "        while not done:\n",
    "            if is_sb3:\n",
    "                action, _ = policy.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action = policy.select_action(obs)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_r += reward\n",
    "\n",
    "        ep_rewards.append(total_r)\n",
    "        env.close()\n",
    "\n",
    "    return ep_rewards\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    sumo_cfg,\n",
    "    ems_day,\n",
    "    pems_day_rl,\n",
    "    meta_rl,\n",
    "    tls_ids,\n",
    "    model_path=\"models/ppo_traffic_day1.zip\",\n",
    "):\n",
    "    # Build one SB3-compatible env\n",
    "    env = SB3TrafficEnv(\n",
    "        sumo_cfg=sumo_cfg,\n",
    "        ems_day=ems_day,\n",
    "        pems_day_rl=pems_day_rl,\n",
    "        meta_rl=meta_rl,\n",
    "        tls_ids=tls_ids,\n",
    "        use_gui=False,\n",
    "        sim_duration_s=3600,\n",
    "    )\n",
    "\n",
    "    # Baseline 1: fixed-time\n",
    "    fixed_agent = FixedTimeController(tls_ids=tls_ids, phase_duration_steps=20, max_phases=4)\n",
    "    fixed_rewards = evaluate_policy(env, fixed_agent, episodes=3, is_sb3=False)\n",
    "    print(\"Fixed-time episode rewards:\", fixed_rewards, \"mean:\", np.mean(fixed_rewards))\n",
    "\n",
    "    # Baseline 2: greedy preemption\n",
    "    greedy_agent = GreedyEVPreemptionController(\n",
    "        tls_ids=tls_ids,\n",
    "        phase_duration_steps=20,\n",
    "        max_phases=4,\n",
    "        tls_phase_map=None  # you can pass real mappings\n",
    "    )\n",
    "    env = SB3TrafficEnv(sumo_cfg, ems_day, pems_day_rl, meta_rl, tls_ids, use_gui=False, sim_duration_s=3600)\n",
    "    greedy_rewards = evaluate_policy(env, greedy_agent, episodes=3, is_sb3=False)\n",
    "    print(\"Greedy preemption episode rewards:\", greedy_rewards, \"mean:\", np.mean(greedy_rewards))\n",
    "\n",
    "    # RL agent\n",
    "    model = PPO.load(model_path)\n",
    "    env = SB3TrafficEnv(sumo_cfg, ems_day, pems_day_rl, meta_rl, tls_ids, use_gui=False, sim_duration_s=3600)\n",
    "    rl_rewards = evaluate_policy(env, model, episodes=3, is_sb3=True)\n",
    "    print(\"RL (PPO) episode rewards:\", rl_rewards, \"mean:\", np.mean(rl_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfdff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00481b11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
